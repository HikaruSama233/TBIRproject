{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task1: Run several analogy solving models with several different representations on the benchmarking analogy dataset and report your findings. \n",
    "\n",
    "Focus on the following questions:\n",
    "\n",
    "1. Is the choice of the analogy model important? Which representations work better with which analogy models?\n",
    "2. Is dimensionality of the representation important when using GloVe vectors?\n",
    "3. What is the computational complexity of the analogy models given the pre-trained vectors?\n",
    "4. What are the typical errors?\n",
    "    \n",
    "    One error is from the testing set. It says London is capital of England but actually it is UK or Britain. So there are about 200 false negative is about that.\n",
    "    Another error is (although I haven't meet it yet) the given word is not in the vocabulary. So it doesn't have a vector to present itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models, matutils\n",
    "import numpy as np\n",
    "import smart_open\n",
    "import os\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def glove2word2vec(glove_filename):\n",
    "    \"\"\"\n",
    "    Convert glove file to word2vec format\n",
    "    The only difference between word2vec format and glove is\n",
    "    word2vec provides number of lines and dimension of word vectors\n",
    "    in first line\n",
    "    This is a short cut for given 4 glove file:\n",
    "        \"glove.6B.50d.txt\", \"glove.6B.100d.txt\", \"glove.6B.200d.txt\", \"glove.6B.300d.txt\"\n",
    "    Other format of glove filename needs other modification of getFirstLineInfo    \n",
    "    \"\"\"\n",
    "    def getFirstLineInfo(glove_filename):\n",
    "        \"\"\"\n",
    "        Calculate the number of lines and dimensions of word vector\n",
    "        \"\"\"\n",
    "        num_lines = sum(1 for line in smart_open.smart_open(glove_filename))\n",
    "        # the file name of glove file contains number of dimensions so we can extract that from file name\n",
    "        dims = glove_filename.split('.')[2].split('d')[0]\n",
    "        return num_lines, dims\n",
    "    \n",
    "    def addFirstLine(glove_filename, word2vec_filename, first_line_info):\n",
    "        \"\"\"\n",
    "        Add information of number of lines and dimensions into the first line\n",
    "        \"\"\"\n",
    "        with smart_open.smart_open(glove_filename, 'rb') as infile:\n",
    "            with smart_open.smart_open(word2vec_filename, 'wb') as outfile:\n",
    "                outfile.write(str(first_line_info) + '\\n')\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "        return word2vec_filename\n",
    "    \n",
    "    word2vec_filename = glove_filename[:-3] + \"word2vec.txt\"\n",
    "    if os.path.isfile(word2vec_filename):\n",
    "        model = models.word2vec.Word2Vec.load_word2vec_format(word2vec_filename)\n",
    "    else:\n",
    "        num_lines, dims = getFirstLineInfo(glove_filename)\n",
    "        first_line = \"{} {}\".format(num_lines, dims)\n",
    "        model_file = addFirstLine(glove_filename, word2vec_filename, first_line)\n",
    "        model = models.word2vec.Word2Vec.load_word2vec_format(model_file)\n",
    "    \n",
    "    # normalize all word vectors\n",
    "    model.init_sims(replace = True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findAnalogy_model1(a, b, c, model): \n",
    "    \"\"\"\n",
    "    addition model\n",
    "    a to b is c to d. a, b, c, d are all words. \n",
    "    d = argmax(cos(d', c-a+b)). \n",
    "    \n",
    "    bonus:\n",
    "        If you want to find the most corrected word of a given word, you can use\n",
    "        findAnalogy_model1(\"empty\", \"empty\", \"given_word\", model)\n",
    "        yes. I know you find out the first two \"empty\" can be replaced by any word but these two must be exactly the same word.\n",
    "    \"\"\"\n",
    "    mixedNormVec = None\n",
    "    all_words = set()\n",
    "    for word in [a, b, c]:\n",
    "        if not word in model.vocab:\n",
    "            raise KeyError(\"word '%s' not in vocabulary\" % word)\n",
    "        else:\n",
    "            all_words.add(model.vocab[word].index)\n",
    "    # normalize the result of b + c - a. prepare for computing cosine similarity\n",
    "    mixedNormVec = matutils.unitvec(model[b] + model[c] - model[a]).astype(np.float32)\n",
    "    \n",
    "    # calculate the cosine similarity between all words (d') and c-a+b\n",
    "    sims = np.dot(model.syn0norm, mixedNormVec)\n",
    "    # find 5 best result which is the highest similarity score\n",
    "    # it is possible that finding the same word as a or b or c \n",
    "    # so we need to give some space for other possible words\n",
    "    best = matutils.argsort(sims, topn = 5, reverse=True)\n",
    "    # ignore words from the input\n",
    "    result = [(model.index2word[sim], float(sims[sim])) for sim in best if sim not in all_words and \"_\" not in model.index2word[sim]]\n",
    "    return result[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findAnalogy_model2(a, b, c, model):\n",
    "    \"\"\"\n",
    "    multiplication model\n",
    "    a to b is c to d. d = argmax(cos(d',c)*cos(d',b)/(cos(d'a)+e))\n",
    "    e = 0.001 to avoid division by zero\n",
    "    \"\"\"\n",
    "    all_words = set()\n",
    "    for word in [a, b, c]:\n",
    "        if not word in model.vocab:\n",
    "            raise KeyError(\"word '%s' not in vocabulary\" % word)\n",
    "        else:\n",
    "            all_words.add(model.vocab[word].index)\n",
    "\n",
    "    sims = (np.dot(model.syn0norm, model[c]) + 1) / 2 * \\\n",
    "        (np.dot(model.syn0norm, model[b]) + 1) / 2 / \\\n",
    "        ((np.dot(model.syn0norm, model[a]) + 1) / 2 + 0.001)\n",
    "    best = matutils.argsort(sims, topn = 5, reverse=True)\n",
    "    # ignore words from the input\n",
    "    result = [(model.index2word[sim], float(sims[sim])) for sim in best if sim not in all_words and \"_\" not in model.index2word[sim]]\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recallOfAnalogyModel(questions, r_model, a_model):\n",
    "    \"\"\"\n",
    "    questions are file path for the test file. In this case, 'questions-words.txt'\n",
    "    r_model is representation model of word vector (word2vec, GloVe)\n",
    "    a_model is analogy model (1, 2). 1 stands for findAnalogy_model1, 2 stands for findAnalogy_model2\n",
    "    \"\"\"\n",
    "    count_correct = 0 # counter for number of correct result\n",
    "    count_total = 0 # count total number of questions\n",
    "    \n",
    "    if a_model == 1:\n",
    "        with open(questions, 'r') as ifile:\n",
    "            for line in ifile:\n",
    "                if line[0] != ':' :\n",
    "                    count_total += 1\n",
    "                    line_sp = line.split()\n",
    "                    result_text = findAnalogy_model1(line_sp[0], line_sp[1], line_sp[2], r_model)                 \n",
    "                    if result_text[0] == line_sp[3]:\n",
    "                        count_correct += 1\n",
    "    elif a_model == 2:\n",
    "        with open(questions, 'r') as ifile:\n",
    "            for line in ifile:\n",
    "                if line[0] != ':' :\n",
    "                    count_total += 1\n",
    "                    line_sp = line.split()\n",
    "                    result_text = findAnalogy_model2(line_sp[0], line_sp[1], line_sp[2], r_model)\n",
    "                    if result_text[0] == line_sp[3]:\n",
    "                        count_correct += 1\n",
    "    else:\n",
    "        raise ValueError(\"invalid analogy model\")\n",
    "    recall = float(count_correct) / float(count_total)\n",
    "    return float('%.4f'% recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pre-trained representation model. Whether load those model at the same time depends on the space of your RAM\n",
    "\n",
    "# Load pre-trained word2vec model from disk\n",
    "word2vec_model = models.word2vec.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative3000.bin', binary = True)\n",
    "\n",
    "# word2vec_model.vocab: 3000000  words for word2vec_model.\n",
    "# each word are represented as a vector with 300 terms\n",
    "# word2vec_model.syn0: matrix for the model\n",
    "# word2vec_model.syn0.shape: check the shape of this matrix\n",
    "\n",
    "# Normalize all vectors in this model. \n",
    "# So we can use dot product to calculate cosine similarity which is more efficient\n",
    "word2vec_model.init_sims(replace = True)\n",
    "# The normalized vectors are stored in model.syn0 and model.syn0norm. These are the same.\n",
    "\n",
    "# Load pre-trained glove model from dist\n",
    "glove50d_model = glove2word2vec('glove.6B.50d.txt')\n",
    "glove100d_model = glove2word2vec('glove.6B.100d.txt')\n",
    "glove200d_model = glove2word2vec('glove.6B.200d.txt')\n",
    "glove300d_model = glove2word2vec('glove.6B.300d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"word2vec & addition model\", recallOfAnalogyModel('questions-words.txt', word2vec_model, 1)\n",
    "print \"word2vec & multiplication model\", recallOfAnalogyModel('questions-words.txt', word2vec_model, 2)\n",
    "print \"glove50d & addition model\", recallOfAnalogyModel('questions-words.txt', glove50d_model, 1)\n",
    "print \"glove50d & multiplication model\", recallOfAnalogyModel('questions-words.txt', glove50d_model, 2)\n",
    "print \"glove100d & addition model\", recallOfAnalogyModel('questions-words.txt', glove100d_model, 1)\n",
    "print \"glove100d & multiplication model\", recallOfAnalogyModel('questions-words.txt', glove100d_model, 2)\n",
    "print \"glove200d & addition model\", recallOfAnalogyModel('questions-words.txt', glove200d_model, 1)\n",
    "print \"glove200d & multiplication model\", recallOfAnalogyModel('questions-words.txt', glove200d_model, 2)\n",
    "print \"glove300d & addition model\", recallOfAnalogyModel('questions-words.txt', glove300d_model, 1)\n",
    "print \"glove300d & multiplication model\", recallOfAnalogyModel('questions-words.txt', glove300d_model, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
