{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task1: Run several analogy solving models with several different representations on the benchmarking analogy dataset and report your findings. \n",
    "\n",
    "Focus on the following questions:\n",
    "\n",
    "1. Is the choice of the analogy model important? Which representations work better with which analogy models?\n",
    "2. Is dimensionality of the representation important when using GloVe vectors?\n",
    "3. What is the computational complexity of the analogy models given the pre-trained vectors?\n",
    "4. What are the typical errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models, matutils\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pre-trained word2vec model from disk\n",
    "word2vec_model = models.word2vec.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative3000.bin', binary = True)\n",
    "\n",
    "# word2vec_model.vocab: 3000000  words for word2vec_model.\n",
    "# each word are represented as a vector with 300 terms\n",
    "# word2vec_model.syn0: matrix for the model\n",
    "# word2vec_model.syn0.shape: check the shape of this matrix\n",
    "\n",
    "# Normalize all vectors in this model. \n",
    "# So we can use dot product to calculate cosine similarity which is more efficient\n",
    "word2vec_model.init_sims()\n",
    "# The normalized vectors are stored in model.syn0 and model.syn0norm. These are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe model from disk\n",
    "glove50d_model = models.word2vec.Word2Vec.load_word2vec_format('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findAnalogy_model1(a, b, c, model): \n",
    "    \"\"\"\n",
    "    addition model\n",
    "    a to b is c to d. a, b, c, d are all words. \n",
    "    d = argmax(cos(d', c-a+b)). \n",
    "    \"\"\"\n",
    "    mixedNormVec = None\n",
    "    all_words = set()\n",
    "    for word in [a, b, c]:\n",
    "        if not word in model.vocab:\n",
    "            raise KeyError(\"word '%s' not in vocabulary\" % word)\n",
    "        else:\n",
    "            all_words.add(model.vocab[word].index)\n",
    "    # normalize the result of b + c - a. prepare for computing cosine similarity\n",
    "    mixedNormVec = matutils.unitvec(model[b] + model[c] - model[a]).astype(np.float32)\n",
    "    \n",
    "    # calculate the cosine similarity between all words (d') and c-a+b\n",
    "    sims = np.dot(model.syn0norm, mixedNormVec)\n",
    "    # find 5 best result which is the highest similarity score\n",
    "    # it is possible that finding the same word as a or b or c \n",
    "    # so we need to give some space for other possible words\n",
    "    best = matutils.argsort(sims, topn = 5, reverse=True)\n",
    "    # ignore words from the input\n",
    "    result = [(model.index2word[sim], float(sims[sim])) for sim in best if sim not in all_words and \"_\" not in model.index2word[sim]]\n",
    "    return result[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findAnalogy_model2(a, b, c, model):\n",
    "    \"\"\"\n",
    "    multiplication model\n",
    "    a to b is c to d. d = argmax(cos(d',c)*cos(d',b)/(cos(d'a)+e))\n",
    "    e = 0.001 to avoid division by zero\n",
    "    \"\"\"\n",
    "    all_words = set()\n",
    "    for word in [a, b, c]:\n",
    "        if not word in model.vocab:\n",
    "            raise KeyError(\"word '%s' not in vocabulary\" % word)\n",
    "        else:\n",
    "            all_words.add(model.vocab[word].index)\n",
    "\n",
    "    sims = (np.dot(model.syn0norm, model[c]) + 1) / 2 * \\\n",
    "        (np.dot(model.syn0norm, model[b]) + 1) / 2 / \\\n",
    "        ((np.dot(model.syn0norm, model[a]) + 1) / 2 + 0.001)\n",
    "    best = matutils.argsort(sims, topn = 5, reverse=True)\n",
    "    # ignore words from the input\n",
    "    result = [(model.index2word[sim], float(sims[sim])) for sim in best if sim not in all_words and \"_\" not in model.index2word[sim]]\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recallOfModel(questions, r_model, a_model):\n",
    "    \"\"\"\n",
    "    questions are file path for the test file. In this case, 'questions-words.txt'\n",
    "    r_model is representation model of word vector (word2vec, GloVe)\n",
    "    a_model is analogy model (1, 2). 1 stands for findAnalogy_model1, 2 stands for findAnalogy_model2\n",
    "    \"\"\"\n",
    "    count_correct = 0 # counter for number of correct result\n",
    "    count_total = 0 # count total number of questions\n",
    "    \n",
    "    if a_model == 1:\n",
    "        with open(questions, 'r') as ifile:\n",
    "            for line in ifile:\n",
    "                if line[0] != ':' :\n",
    "                    count_total += 1\n",
    "                    line_sp = line.split()\n",
    "                    result_text = findAnalogy_model1(line_sp[0], line_sp[1], line_sp[2], r_model)                 \n",
    "                    if result_text[0] == line_sp[3]:\n",
    "                        count_correct += 1\n",
    "    elif a_model == 2:\n",
    "        with open(questions, 'r') as ifile:\n",
    "            for line in ifile:\n",
    "                if line[0] != ':' :\n",
    "                    count_total += 1\n",
    "                    line_sp = line.split()\n",
    "                    result_text = findAnalogy_model2(line_sp[0], line_sp[1], line_sp[2], r_model)\n",
    "                    if result_text[0] == line_sp[3]:\n",
    "                        count_correct += 1\n",
    "    else:\n",
    "        raise ValueError(\"invalid analogy model\")\n",
    "    recall = float(count_correct) / float(count_total)\n",
    "    return float('%.4f'% recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print [\"word2vec & addition model\", recallOfModel('questions-words.txt', word2vec_model, 1)]\n",
    "print [\"word2vec & multiplication model\", recallOfModel('questions-words.txt', word2vec_model, 2)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
